{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN lies in how they measure distance between data points:\n",
    "\n",
    "Euclidean distance measures the straight-line distance between two points in Euclidean space, calculated as the square root of the sum of squared differences between corresponding coordinates.\n",
    "Manhattan distance (also known as city block distance or L1 distance) measures the distance as the sum of the absolute differences between corresponding coordinates.\n",
    "This difference affects KNN because the choice of distance metric influences how neighboring points are defined. For example, Euclidean distance considers diagonal movements, which might be appropriate for continuous features with underlying geometric interpretations. On the other hand, Manhattan distance might be more suitable for cases where features are discrete or where directions matter less, such as in text classification. Choosing the appropriate distance metric can affect the model's performance by altering the way the model perceives the data's geometry.\n",
    "Q2. Choosing the optimal value of k for a KNN classifier or regressor often involves experimentation and validation techniques like cross-validation. Some common techniques for determining the optimal k value include:\n",
    "\n",
    "Grid search: Testing a range of k values and selecting the one that yields the best performance on a validation set.\n",
    "Cross-validation: Splitting the training data into multiple folds, training the model on different subsets, and evaluating performance on validation data to find the best k value.\n",
    "Domain knowledge: Prior knowledge about the problem or data characteristics may provide insights into an appropriate range of k values.\n",
    "Q3. The choice of distance metric can significantly affect the performance of a KNN classifier or regressor. Euclidean distance is sensitive to the scale of the features and assumes that features are continuous and normally distributed. In contrast, Manhattan distance is less affected by outliers and can be more robust to differences in feature scales. The choice between these distance metrics depends on the nature of the data and the problem at hand. For example, if the features are categorical or non-normally distributed, Manhattan distance might be more appropriate. On the other hand, if the features have a clear geometric interpretation and are continuous, Euclidean distance might be a better choice.\n",
    "\n",
    "Q4. Some common hyperparameters in KNN classifiers and regressors include:\n",
    "\n",
    "k: The number of neighbors to consider.\n",
    "Distance metric: The measure used to compute the distance between data points.\n",
    "Weighting scheme: The method used to weight the contributions of neighbors (e.g., uniform or distance-based weights).\n",
    "Tuning these hyperparameters can improve model performance by finding the optimal balance between bias and variance. Techniques like grid search or random search can be used to systematically explore the hyperparameter space and find the combination that yields the best performance.\n",
    "Q5. The size of the training set can significantly affect the performance of a KNN classifier or regressor. With a small training set, the model may struggle to capture the underlying patterns in the data and may be prone to overfitting. Conversely, with a large training set, the model may become computationally expensive to train and may suffer from high variance. Techniques such as cross-validation can help determine an appropriate training set size by evaluating model performance on different subsets of the data.\n",
    "\n",
    "Q6. Some potential drawbacks of using KNN as a classifier or regressor include:\n",
    "\n",
    "Computationally expensive: KNN requires storing all training data and computing distances during prediction, making it computationally expensive, especially for large datasets.\n",
    "Sensitivity to irrelevant features: KNN considers all features equally, so irrelevant or noisy features can adversely affect model performance.\n",
    "Curse of dimensionality: KNN's performance can degrade with high-dimensional data due to the curse of dimensionality, where the distance between points becomes less meaningful in high-dimensional space.\n",
    "To overcome these drawbacks, techniques such as dimensionality reduction (e.g., PCA), feature selection, and feature scaling can be applied to improve the performance of the model and reduce computational complexity. Additionally, using algorithms that are less sensitive to irrelevant features, such as decision trees or ensemble methods, may be preferable in certain scenarios.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
